import os
import json
import certifi
import requests
import re
from datetime import datetime, timedelta
from dotenv import load_dotenv
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent

load_dotenv()
os.environ["SSL_CERT_FILE"] = certifi.where()


# ============================================================
# AUTH
# ============================================================
def get_azure_access_token():
    tenant = os.getenv("AZURE_TENANT_ID")
    client_id = os.getenv("AZURE_CLIENT_ID")
    client_secret = os.getenv("AZURE_CLIENT_SECRET")

    url = f"https://login.microsoftonline.com/{tenant}/oauth2/v2.0/token"

    payload = {
        "client_id": client_id,
        "client_secret": client_secret,
        "scope": "https://analysis.windows.net/powerbi/api/.default",
        "grant_type": "client_credentials",
    }

    response = requests.post(url, data=payload)
    response.raise_for_status()
    return response.json()["access_token"]


# ============================================================
# EXECUTE DAX
# ============================================================
def execute_dax_query(dax_query: str):
    dataset_id = os.getenv("POWERBI_DATASET_ID")
    token = get_azure_access_token()

    url = f"https://api.powerbi.com/v1.0/myorg/datasets/{dataset_id}/executeQueries"

    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
    }

    body = {
        "queries": [{"query": dax_query}],
        "serializerSettings": {"includeNulls": True},
    }

    response = requests.post(url, headers=headers, json=body, timeout=60)
    response.raise_for_status()
    return response.json()


# ============================================================
# STRICT TIME PARSER
# ============================================================
def parse_time_range(question: str):
    q = question.lower()

    pattern = r"last\s+(\d+)?\s*(day|days|week|weeks|month|months|year|years)"
    match = re.search(pattern, q)

    if match:
        value = int(match.group(1)) if match.group(1) else 1
        unit = match.group(2)
        return value, unit

    if "yesterday" in q:
        return 1, "days"

    return None, None


# ============================================================
# BUILD SAFE DATE FILTER
# ============================================================
def build_date_filter(question: str):

    value, unit = parse_time_range(question)

    if value is None:
        return None

    today = datetime.today()

    if "day" in unit:
        start_date = today - timedelta(days=value)
    elif "week" in unit:
        start_date = today - timedelta(weeks=value)
    elif "month" in unit:
        start_date = today - timedelta(days=30 * value)
    elif "year" in unit:
        start_date = today - timedelta(days=365 * value)
    else:
        return None

    return f"'cost'[date] >= DATE({start_date.year},{start_date.month},{start_date.day})"


# ============================================================
# HARD DAX SANITIZER (NO EXCEPTIONS)
# ============================================================
def sanitize_dax(dax: str, question: str):

    # Remove unsupported constructs
    dax = re.sub(r"\bVAR\b.*?\n", "", dax, flags=re.IGNORECASE)
    dax = re.sub(r"\bRETURN\b", "", dax, flags=re.IGNORECASE)
    dax = re.sub(r"FILTER\s*\(.*?\)", "", dax, flags=re.DOTALL | re.IGNORECASE)
    dax = re.sub(r"DATESINPERIOD\s*\(.*?\)", "", dax, flags=re.DOTALL | re.IGNORECASE)
    dax = re.sub(r"TODAY\(\)", "", dax, flags=re.IGNORECASE)
    dax = re.sub(r"LASTDATE\s*\(.*?\)", "", dax, flags=re.DOTALL | re.IGNORECASE)

    dax = dax.replace("SUMMARIZE(", "SUMMARIZECOLUMNS(")

    if not dax.strip().upper().startswith("EVALUATE"):
        dax = "EVALUATE\n" + dax

    # Inject safe date filter
    date_filter = build_date_filter(question)

    if date_filter:
        dax = dax.replace(")", f", {date_filter})", 1)

    return dax.strip()


# ============================================================
# EXTRACT DAX FROM LLM
# ============================================================
def extract_dax_from_response(llm_text: str):
    match = re.search(r"```DAX\s*(.*?)```", llm_text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return None


# ============================================================
# EXTRACT MCP SCHEMA
# ============================================================
def extract_mcp_schema(result):
    for msg in result["messages"]:
        if msg.type == "tool":
            artifact = msg.artifact
            if artifact and "structured_content" in artifact:
                return artifact["structured_content"]
    return None


# ============================================================
# CONTEXT HOLDER
# ============================================================
class FinOpsContext:
    def __init__(self, model, schema):
        self.model = model
        self.schema = schema


# ============================================================
# INITIAL SETUP
# ============================================================
async def finops_setup():

    print("üîê Authenticating...")
    token = get_azure_access_token()

    servers = {
        "powerbi-remote": {
            "transport": "http",
            "url": "https://api.fabric.microsoft.com/v1/mcp/powerbi",
            "headers": {"Authorization": f"Bearer {token}"}
        }
    }

    client = MultiServerMCPClient(servers)
    tools = await client.get_tools()

    model = init_chat_model(
        os.getenv("MODEL"),
        model_provider="openai",
        api_key=os.getenv("GITHUB_TOKEN"),
        base_url=os.getenv("ENDPOINT"),
    )

    system_prompt = """
You are a FinOps analytics engine.

Rules:
- Generate ONLY SUMMARIZECOLUMNS DAX.
- Never use VAR, RETURN, FILTER, DATESINPERIOD, LASTDATE, TODAY.
- Time filtering will be handled externally.
- Only focus on aggregation and grouping.
"""

    agent_graph = create_agent(model, tools, system_prompt=system_prompt, debug=False)

    workspace_id = os.getenv("POWERBI_WORKSPACE_ID")
    dataset_id = os.getenv("POWERBI_DATASET_ID")

    schema_query = f"""
Inspect the semantic model.
Workspace ID: {workspace_id}
Dataset ID: {dataset_id}
"""

    result = await agent_graph.ainvoke(
        {"messages": [{"role": "user", "content": schema_query}]}
    )

    schema = extract_mcp_schema(result)

    print("‚úÖ Schema loaded")
    return FinOpsContext(model, schema)


# ============================================================
# CHAT FUNCTION
# ============================================================
async def chat_instance_stream(user_question, context: FinOpsContext):

    schema_json = json.dumps(context.schema, indent=2)

    prompt = (
        "Generate SAFE executeQueries-compatible DAX.\n"
        "Use SUMMARIZECOLUMNS only.\n\n"
        "Schema:\n" + schema_json +
        "\n\nUser Question:\n" + user_question
    )

    response = await context.model.ainvoke(prompt)
    llm_text = response.content

    dax_query = extract_dax_from_response(llm_text)

    if dax_query:
        try:
            safe_dax = sanitize_dax(dax_query, user_question)

            print("\nExecuting Safe DAX:\n", safe_dax)

            result = execute_dax_query(safe_dax)

            return llm_text + "\n\n---\n### ‚úÖ Live Result\n" + json.dumps(result, indent=2)

        except Exception as e:
            return llm_text + f"\n\n‚ö†Ô∏è Execution failed: {str(e)}"

    return llm_text
