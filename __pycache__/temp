import os
import asyncio
import certifi
import requests
from dotenv import load_dotenv

from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.chat_models import init_chat_model

load_dotenv()

# Fix SSL issues in corporate environments
os.environ["SSL_CERT_FILE"] = certifi.where()


def get_azure_access_token():
    """
    Generates Azure AD token using Service Principal
    """
    tenant = os.getenv("AZURE_TENANT_ID")
    client_id = os.getenv("AZURE_CLIENT_ID")
    client_secret = os.getenv("AZURE_CLIENT_SECRET")

    if not tenant or not client_id or not client_secret:
        raise ValueError("Azure credentials missing in .env")

    url = f"https://login.microsoftonline.com/{tenant}/oauth2/v2.0/token"

    payload = {
        "client_id": client_id,
        "client_secret": client_secret,
        "scope": "https://analysis.windows.net/powerbi/api/.default",
        "grant_type": "client_credentials",
    }

    response = requests.post(url, data=payload)
    response.raise_for_status()

    token = response.json()["access_token"]
    print("‚úÖ Azure token generated successfully")
    return token

async def run_mcp_reasoning(model_with_tools, client, messages):
    """
    Executes the MCP reasoning loop:
    LLM ‚Üí Tool Call ‚Üí Execute ‚Üí Feed Result Back ‚Üí Repeat
    """

    while True:
        response = await model_with_tools.ainvoke(messages)

        # If no tool calls, we are done
        if not response.tool_calls:
            return response

        for call in response.tool_calls:
            tool_name = call["name"]
            tool_args = call["args"]

            print(f"\n‚öôÔ∏è Executing MCP Tool: {tool_name}")
            print(f"Arguments: {tool_args}\n")

            # Find the MCP tool
            tool = next(t for t in await client.get_tools() if t.name == tool_name)

            # Execute against Fabric
            tool_result = await tool.ainvoke(tool_args)

            print(f"‚úÖ Tool Result Received ({tool_name})")

            # Send result back to LLM
            messages.append(response)
            messages.append(
                ToolMessage(
                    content=str(tool_result),
                    tool_call_id=call["id"],
                )
            )


async def main():
    print("üîê Generating Azure access token...")
    token = get_azure_access_token()

    # Connect to Fabric MCP Endpoint
    servers = {
        "powerbi-remote": {
            "transport": "http",
            "url": "https://api.fabric.microsoft.com/v1/mcp/powerbi",
            "headers": {
                "Authorization": f"Bearer {token}"
            }
        }
    }

    print("üîå Connecting to MCP server...")
    client = MultiServerMCPClient(servers)

    tools = await client.get_tools()

    print(f"‚úÖ Connected to MCP. Discovered {len(tools)} tools\n")

    # Print tool names so you understand what MCP exposes
    print("üõ† Available MCP Tools:")
    for t in tools:
        print(" -", t.name)

    # Initialize LLM (NO AGENT)
    model = init_chat_model(
        os.getenv("MODEL"),
        model_provider="openai",
        api_key=os.getenv("GITHUB_TOKEN"),
        base_url=os.getenv("ENDPOINT"),
    )

    # üî¥ Bind tools directly ‚Äî THIS is the key difference
    model_with_tools = model.bind_tools(tools)

    workspace_id = os.getenv("POWERBI_WORKSPACE_ID")
    dataset_id = os.getenv("POWERBI_DATASET_ID")

    if not workspace_id or not dataset_id:
        raise ValueError("Workspace/Dataset ID missing in .env")

    # Force MCP schema inspection first
    user_query = f"""
You are connected to a Power BI Semantic Model via MCP.

You MUST use MCP tools to inspect the dataset.

Workspace ID: {workspace_id}
Dataset ID: {dataset_id}

Steps:
1. Discover available tables
2. Inspect table schema
3. List measures

DO NOT answer from knowledge.
ONLY answer using MCP tool results.
"""

    messages = [
        SystemMessage(content="""
                      You are a FinOps analyst working ONLY through MCP tools.
                      Never answer directly.
                      Always inspect the semantic model using available tools.
                      """),
                      HumanMessage(content=user_query)
    ]

    print("\nüìä Inspecting semantic model via MCP...\n")

    # Direct invocation ‚Äî not an agent
    result = await run_mcp_reasoning(model_with_tools, client, messages)

    print("\n================ MCP RESPONSE ================\n")
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
